-- see: https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d
{ X âŸœ âŸ¨âŸ¨0,0âŸ©,âŸ¨0,1âŸ©,âŸ¨1,0âŸ©,âŸ¨1,1âŸ©âŸ©;
  Y âŸœ âŸ¨0,1,1,0âŸ©;
  sigmoid â† [1+â„¯(_x)];
  sDdx â† [x*(1-x)];
  dot â‡ [(+)/1 0 ((*)`x y)]; sum â‡ [(+)/1 0 x];
  vmul â‡ Î»A.Î»x. (dot x)`{1âˆ˜[2]} A;
  forward â† Î»wh.Î»wo.Î»bh.Î»bo.
    -- ho: 4x2
    { ho â† sigmoid'2 ([(+)` bh x]'1 (X%.wh))
    -- prediction: 4
    ; prediction â† sigmoid'1 ((+bo)'1 (vmul ho wo))
    ; (ho,prediction)
    };
  -- wh: 2x2 wo: 2 bh: 2 bo: (scalar)
  train â† Î»inp.
    { wh â† inp->1; wo â† inp->2; bh â† inp->3; bo â† inp->4
    ; o â† forward wh wo bh bo
    ; ho âŸœ o->1; prediction âŸœ o->2
    ; l1E â† (-)`prediction Y
    ; l1Î” â† (*)` (sDdx'1 prediction) l1E -- 4
    ; he â† l1Î” (*)âŠ— wo -- 4x2
    ; hÎ” â† (*)`{0,0} (sDdx'2 ho) he -- 4x2
    ; wha â† (+)`{0,0} wh ((|:X)%.hÎ”)
    ; woa â† vmul (|:ho) l1Î”
    ; bha â† sum'1 ((<|)`{0,1} bh hÎ”)
    ; boa â† bo + sum l1Î”
    ; (wha,woa,bha,boa)
    };
  wh âŸœ {gâ†\x.[ğ”¯_1 1]'1 (irange 1 2 1);g'1(irange 1 2 1)};
  wo âŸœ [ğ”¯_1 1]'1 (irange 1 2 1);
  bh âŸœ [ğ”¯_1 1]'1 (irange 1 2 1);
  bo âŸœ ğ”¯_1 1;
  (train^:10000) (wh,wo,bh,bo)
}
