-- see: https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d
{ X âŸœ âŸ¨âŸ¨0,0âŸ©,âŸ¨0,1âŸ©,âŸ¨1,0âŸ©,âŸ¨1,1âŸ©âŸ©;
  Y âŸœ âŸ¨0,1,1,0âŸ©;
  sigmoid â† [1%(1+â„¯(_x))];
  sDdx â† [x*(1-x)];
  sum â‡ [(+)/x];
  forward â† Î»wh.Î»wo.Î»bh.Î»bo.
    -- ho: 4x2
    { ho â† sigmoid`{0} ([(+)`bh x]'(X%.wh))
    -- prediction: 4
    ; prediction â† sigmoid'((+bo)'(ho%:wo))
    ; (ho,prediction)
    };
  -- wh: 2x2 wo: 2 bh: 2 bo: (scalar)
  train â† Î»inp.
    { wh â† inp->1; wo â† inp->2; bh â† inp->3; bo â† inp->4
    ; o â† forward wh wo bh bo
    ; ho âŸœ o->1; prediction âŸœ o->2
    ; l1E â† (-)`prediction Y
    ; l1Î” â† (*)`(sDdx'prediction) l1E -- 4
    ; he â† l1Î” (*)âŠ— wo -- 4x2
    ; hÎ” â† (*)`{0,0} (sDdx`{0} ho) he -- 4x2
    ; wha â† (+)`{0,0} wh ((|:X)%.hÎ”)
    ; woa â† (+)`wo ((|:ho)%:l1Î”)
    ; bha â† [(+)/â‚’ x y]`{0,1} bh hÎ”
    ; boa â† bo + sum l1Î”
    ; (wha,woa,bha,boa)
    };
  wh âŸœ ğ”¯_1 1;wo âŸœ ğ”¯_1 1;bh âŸœ ğ”¯_1 1;bo âŸœ ğ”¯_1 1;
  train^:10000 (wh,wo,bh,bo)
}
